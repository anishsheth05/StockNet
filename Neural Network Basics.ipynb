{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few Basic Examples of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "for j in range(60000):\n",
    "    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))\n",
    "    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))\n",
    "    l2_delta = (y - l2)*(l2*(1-l2))\n",
    "    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += X.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer Version w/ Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[0.00966449]\n",
      " [0.00786506]\n",
      " [0.99358898]\n",
      " [0.99211957]]\n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in range(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print (\"Output After Training:\")\n",
    "print (l1)\n",
    "print(syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style='font-size:15px'>\n",
    "    <tr>\n",
    "        <th>Variable</th>\n",
    "        <th>Definition</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>X</td>\n",
    "        <td>Input training dataset where each row is a training example</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>y</td>\n",
    "        <td>Output training dataset where each row is a training example</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>l0</td>\n",
    "        <td>First Layer of the Network, specified by training data</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>l1</td>\n",
    "        <td>Second Layer of the Network, also known as the hidden layer</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>syn0</td>\n",
    "        <td>Synapse 0, the set of weights between and connecting l0 and l1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>*</td>\n",
    "        <td>Elementwise multiplication, aka two vectors multiplying 1-to-1 producing a vector of equal size</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>-</td>\n",
    "        <td>Elementwive subtraction, aka two vectors subtracting 1-to-1 producing a vector of equal size</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>X.dot(y)</td>\n",
    "        <td>Given X and y as vectors, this would be the vector dot product. If they are matricies, it would be a matrix-matrix multiplication. If only one is a matrix, then it's matrix-vector multiplication.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:15px'>The above function is the \"nonlinearity\" function, which can convert numbers to probabilities for us. It can also return the derivative of the sigmoid, when deriv=True - this is very easy, given the derivate of the sigmoid for f(x) is just x\\*(x-1), making things very efficient. (The derivative is of course the slope of the sigmoid at a point on the line; practically, the derivative gives the slope of the sigmoid for point x as x\\*(x-1). The sigmoid's function is f(x) = 1/(1+e^(-x))</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:15px'>The above lines of code are simply initializing the input and output datasets. The first array is the input dataset, and the second array is the output dataset. Each row of the input and outputs corresponds to each other - row one for X and y are together the inputs which correspond to the output; this pattern continues on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:15px'>The above code is useful when randomizing but wanting consistent results, because it makes the np.random.random return randomly distributed but consistently randomly distributed numbers.<b>A good practice in general.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34093502]\n",
      " [-0.1653904 ]\n",
      " [ 0.11737966]]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "print(syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:15px'>This initializes synapse 0 to a shape (3, 1) array of weights. Since the second layer is the output, which is only 1 node, all we need is a set of weights to connect the 3 nodes of the first layer to the 1 node of the second, hence 3 weights. The weights themselves are random and distributed around a mean of 0. <b>Initializing weights with a mean of zero is a best practice</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<body>\n",
    "    <p style='font-size:15px'>Finally, the actual training part of the code is above. The very first line just loops the code 10,000 times to optimize the code to the dataset.</p>\n",
    "    <p style='font-size:15px'>The next line of code set l0 to X, blatently setting up the first layer as the inputs.</p>\n",
    "    <p style='font-size:15px'>The next line of code lets up the second layer, l0, as the dot product of l0 and syn0, put through the sigmoid function. This does the multiplication/propagation while also getting the probabiliy through the sigmoid.</p>\n",
    "    <p style='font-size:15px'>The next line calculates the error between the prediction and actual output. This is useful for adjusting later.</p>\n",
    "    <p style='font-size:15px'>The next line is the part where the recommended adjustment is calculated, aka the l1 delta (change in l1). This is simply the previous line's error calculation multiplied by the derivate of the first layer's values. This is (I think) a one on one multiplication, with the first row's values mutliplyed by each other, so-on and so-forth.</p>\n",
    "    <p style='font-size:15px'>This final line updates syn0, or the weights. It gets the update per weight by performing the dot product of l0.T and l1_delta; in other words, the weights are updated by the dot product of the transposed inputs and the recommended adjustment.</p>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following Youtube Series\n",
    "https://www.youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron, Essentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1]\n",
    "bias = 2\n",
    "\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, An Entire Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights1 = [0.2,0.8, -0.5, 1]\n",
    "weights2 = [0.5,-0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# cleaner version in python\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [[0.2,0.8, -0.5, 1],\n",
    "           [0.5,-0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "#single neuron - undestand dot product\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [0.2,0.8, -0.5, 1]\n",
    "\n",
    "bias = 2\n",
    "\n",
    "# in the case of vector * vector dot product, the order of weights & inputs doesn't matter -\n",
    "# however, with matrix * vector dot product (which it is with mutiple neurons) it does\n",
    "output = np.dot(weights, inputs) + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "# layer dot product\n",
    "inputs = [1, 2, 3, 2.5]                   # the number of inputs is the same\n",
    "\n",
    "weights = [[0.2,0.8, -0.5, 1],            # each one of these list of numbers within\n",
    "           [0.5,-0.91, 0.26, -0.5],       # the list are the weights for one neuron\n",
    "           [-0.26, -0.27, 0.17, 0.87]]    # in total 4 weights per input, for each neuron\n",
    "\n",
    "\n",
    "biases = [2, 3, 0.5]                      # also need a bias per neuron, hence the list of biases\n",
    "\n",
    "# doing np.dot(inputs, weights) doesn't even work - throws error b/c can't multiply vector by matrix,\n",
    "# has to be the other way around\n",
    "output = np.dot(weights, inputs) + biases\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batches, Layers, and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "# using batches, 1 layer\n",
    "inputs = [[1, 2, 3, 2.5],                 # now we are passing multiple input lists\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2,0.8, -0.5, 1],\n",
    "           [0.5,-0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "# Here we need to do inputs first, and then weights because we are transposing the weights\n",
    "# so that matrix-matrix multiplication works - need to do a (3, 4) array times a (4, 3) array\n",
    "outputs = np.dot(inputs, np.array(weights).T) + biases      # to return the final (3, 3) array\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031   0.53225 -2.03875]\n",
      " [ 0.2434  -2.6012  -5.7633 ]\n",
      " [-0.99314  1.4297  -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "# using batches, 2 layers\n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2,0.8, -0.5, 1],\n",
    "           [0.5,-0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "           [-0.5, 0.12, 0.33],\n",
    "           [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# now as an object (with inputs as X - training dataset)\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    \n",
    "layer1 = Layer_Dense(4, 5)\n",
    "layer2 = Layer_Dense(5, 2)\n",
    "\n",
    "layer1.forward(X)\n",
    "#print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally: Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# rectified linear example (ReLU)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "\n",
    "'''\n",
    "for i in inputs:\n",
    "    if i>0:\n",
    "        output.append(i)\n",
    "    if i<=0:\n",
    "        output.append(0)\n",
    "'''\n",
    "# or instead:\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now adding the activation function as an object\n",
    "np.random.seed(0)\n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.max(0, inputs)\n",
    "\n",
    "\n",
    "layer1 = Layer_Dense(4, 5)\n",
    "layer2 = Layer_Dense(5, 2)\n",
    "\n",
    "layer1.forward(X)\n",
    "#print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
